{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d30860",
   "metadata": {
    "id": "97d30860"
   },
   "source": [
    "## Task 1: Analysing Pre-trained Word Embeddings (6 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b79dc34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1b79dc34",
    "outputId": "ff758ccf-b613-462e-c645-4908271beab6"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.50d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtutorial2_task1\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_glove_model\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m load_glove_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglove.6B.50d.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(vocab)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m words found with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vector size!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Neural Networks for NLP Assignments\\Assignment 2\\Exercise 2 Question\\Tutorial 2\\tutorial2_task1.py:14\u001b[0m, in \u001b[0;36mload_glove_model\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mFollowing function returns pretrained glove model in the dictionary format.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThe keys will be all the words in vocabulary.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03mThe value will be the corresponding vector.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m     16\u001b[0m         split_line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.50d.txt'"
     ]
    }
   ],
   "source": [
    "from tutorial2_task1 import load_glove_model\n",
    "\n",
    "model = load_glove_model(\"glove.6B.50d.txt\")\n",
    "vocab = list(model.keys())\n",
    "\n",
    "print(f\"{len(vocab)} words found with {len(model['the'])} vector size!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b37c8",
   "metadata": {},
   "source": [
    "### Finding Distance Between Words\n",
    "\n",
    "Let's explore whether the pre-trained word embeddings accurately reflect the intuitive similarity between words. For instance, we would expect the word \"dog\" to be more similar to \"cat\" (as both are pet animals) than to a word like \"ball.\" We can verify this assumption using the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4288b62f",
   "metadata": {
    "id": "4288b62f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtutorial2_task1\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_distance\n\u001b[1;32m----> 3\u001b[0m d_dog_cat \u001b[38;5;241m=\u001b[39m cosine_distance(model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdog\u001b[39m\u001b[38;5;124m\"\u001b[39m], model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m d_dog_ball \u001b[38;5;241m=\u001b[39m cosine_distance(model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdog\u001b[39m\u001b[38;5;124m\"\u001b[39m], model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mball\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistance between dog and cat: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_dog_cat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from tutorial2_task1 import cosine_distance\n",
    "\n",
    "d_dog_cat = cosine_distance(model[\"dog\"], model[\"cat\"])\n",
    "d_dog_ball = cosine_distance(model[\"dog\"], model[\"ball\"])\n",
    "\n",
    "print(f\"Distance between dog and cat: {d_dog_cat}\")\n",
    "print(f\"Distance between dog and bear: {d_dog_ball}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a3281-22f4-4acb-8aa4-8c439e83c6df",
   "metadata": {},
   "source": [
    "Having established the ability to measure the distance between two words using their word embeddings, we can now extend this approach to compare a specific word against all words in our pre-trained model. For instance, which words are most similar to \"awesome\" in this context? Let's find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f18b46-21cf-451d-8edb-ae77e2327769",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtutorial2_task1\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m most_similar\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtutorial2_task1\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_distance, euclidean_distance\n\u001b[1;32m----> 4\u001b[0m similar_words_cosine \u001b[38;5;241m=\u001b[39m most_similar(model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawesome\u001b[39m\u001b[38;5;124m\"\u001b[39m], model, cosine_distance)\n\u001b[0;32m      5\u001b[0m similar_words_euclidean \u001b[38;5;241m=\u001b[39m most_similar(model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawesome\u001b[39m\u001b[38;5;124m\"\u001b[39m], model, euclidean_distance)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCosine distance:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from tutorial2_task1 import most_similar\n",
    "from tutorial2_task1 import cosine_distance, euclidean_distance\n",
    "\n",
    "similar_words_cosine = most_similar(model[\"awesome\"], model, cosine_distance)\n",
    "similar_words_euclidean = most_similar(model[\"awesome\"], model, euclidean_distance)\n",
    "\n",
    "print(\"Cosine distance:\")\n",
    "print(similar_words_cosine)\n",
    "\n",
    "print(\"Euclidean distance:\")\n",
    "print(similar_words_euclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c14d054",
   "metadata": {},
   "source": [
    "### Compare Similarity Between Groups of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1cc8744",
   "metadata": {
    "id": "d1cc8744"
   },
   "outputs": [],
   "source": [
    "common_words = [\n",
    "    # colors\n",
    "    \"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\",\n",
    "    \"pink\", \"brown\", \"black\", \"grey\", \"white\", \"violet\", \n",
    "    # months\n",
    "    \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \n",
    "    \"july\", \"august\", \"september\", \"october\", \"november\", \"december\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644fc25e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "644fc25e",
    "outputId": "96ad4450-23d5-40f8-f70f-6ca6a67aef34"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtutorial2_task1\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_similarities\n\u001b[1;32m----> 3\u001b[0m show_similarities(common_words, model, cosine_distance)\n\u001b[0;32m      4\u001b[0m show_similarities(common_words, model, euclidean_distance)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from tutorial2_task1 import show_similarities\n",
    "\n",
    "show_similarities(common_words, model, cosine_distance)\n",
    "show_similarities(common_words, model, euclidean_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4c3dd",
   "metadata": {
    "id": "b3b4c3dd"
   },
   "source": [
    "Do the word embeddings agree with you assumption of relatedness between words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86796cb5-b9d3-4e16-9fd4-429d6f390313",
   "metadata": {},
   "source": [
    "Your answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8860a2d7",
   "metadata": {},
   "source": [
    "### Vector Arithmetic for Analogy Solving\n",
    "\n",
    "What happens if we subtract 'man' from 'king'? This is like asking the model to take the concept of 'king', and remove 'maleness' from it. What you're left with (in theory) is the concept of royalty or a ruler without the gender association. Then, by adding the vector for 'woman', you're essentially asking the model to provide a word that is similar to a 'king', but with a feminine aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6808f9",
   "metadata": {
    "id": "6d6808f9"
   },
   "outputs": [],
   "source": [
    "vec = model[\"king\"] - model[\"man\"] + model[\"woman\"]\n",
    "similar_words = most_similar(vec, model, euclidean_distance, ignore_vec=np.array([model[\"king\"]]))\n",
    "\n",
    "print(similar_words)\n",
    "print(most_similar(model[\"king\"], model, euclidean_distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56de51-7e83-40b9-931a-7bb8742ec25c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "c1ce2838",
    "outputId": "f86b1354-eacf-4633-c0dc-b4bf597e8612"
   },
   "source": [
    "More analogies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb327f1-dd6e-4c61-8c99-408465685c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial2_task1 import analogies\n",
    "\n",
    "# add more analogies here\n",
    "\n",
    "print(analogies(\"man\", \"king\", \"woman\", model))\n",
    "# print(analogies(\"brazil\", \"pele\", \"argentina\", model))\n",
    "# print(analogies(\"america\", \"washington\", \"germany\", model))\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e664efe7",
   "metadata": {
    "id": "94f68ba5"
   },
   "source": [
    "## Task 2 (Computing Word Embeddings - Word2Vec (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f7d28-7386-42ee-aca4-2552233f7a51",
   "metadata": {},
   "source": [
    "### Preparing Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b65b98-b8f5-4c91-a70b-ec2f91740c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial2_task2 import tokenize\n",
    "\n",
    "tokens = tokenize(\"tutorial2.txt\")\n",
    "\n",
    "print(tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e370e885-7dd0-4f91-8e6e-2cc42790d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial2_task2 import generate_training_data\n",
    "\n",
    "context_window_size = 1\n",
    "\n",
    "center_words, context_words = generate_training_data(tokens, context_window_size=context_window_size)\n",
    "\n",
    "print(center_words[0], context_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d29efe4",
   "metadata": {},
   "source": [
    "#### Train the following CBOW and SkipGram models by completing their 'forward' functions. Use appropriate activation functions wherever necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0852adf5-e4a9-4b6a-9dd9-54afd23384d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from tutorial2_task2 import generate_mappings\n",
    "\n",
    "vocabulary, word2id, id2word = generate_mappings(tokens)\n",
    "\n",
    "# defining hyperparameters\n",
    "embedding_dim = 50\n",
    "num_epochs = 200\n",
    "context_size = 2 * context_window_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b895e75-6ba3-44e0-872f-8e719dbadd35",
   "metadata": {},
   "source": [
    "##### Training CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14b2fc-2fe6-42df-a142-270666f39a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial2_task2 import CBOW, train_word2vec\n",
    "\n",
    "cbow_inputs = torch.tensor([[word2id[w] for w in row] for row in context_words])\n",
    "cbow_targets = torch.tensor([[word2id[w]] for w in center_words])\n",
    "\n",
    "cbow_model = CBOW(len(vocabulary), context_size, embedding_dim)\n",
    "cbow_loss = train_word2vec(cbow_model, cbow_inputs, cbow_targets, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f5b014-2584-4312-b412-8d1dad8b2d0d",
   "metadata": {},
   "source": [
    "##### Training SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db8d32-db12-4c0f-ac4a-3fd7a755aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial2_task2 import SkipGram, train_word2vec\n",
    "\n",
    "skipgram_inputs = cbow_targets\n",
    "skipgram_targets = cbow_inputs\n",
    "\n",
    "skipgram_model = SkipGram(len(vocabulary), context_size, embedding_dim)\n",
    "skipgram_loss = train_word2vec(skipgram_model, skipgram_inputs, skipgram_targets, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2f356-a172-453f-a157-ab549e2f5539",
   "metadata": {},
   "source": [
    "##### Comparing Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b553cbfe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "b553cbfe",
    "outputId": "a265345d-605f-4190-8e48-ed68fc2c327f"
   },
   "outputs": [],
   "source": [
    "from tutorial2_task2 import plot_loss\n",
    "\n",
    "plot_loss(\n",
    "    'CBOW vs SkipGram Training Loss', \n",
    "    [\n",
    "        (cbow_loss, \"CBOW\", \"r\"), \n",
    "        (skipgram_loss, \"SkipGram\", \"b\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a99e5f",
   "metadata": {
    "id": "15a99e5f"
   },
   "source": [
    "#### Retrain Using a Larger Context Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a877cadd",
   "metadata": {
    "id": "a877cadd"
   },
   "outputs": [],
   "source": [
    "# TODO: choose one and comment the other\n",
    "model_cls = CBOW\n",
    "# model_cls = SkipGram\n",
    "\n",
    "context_window_size = 2\n",
    "context_size = 2 * context_window_size\n",
    "\n",
    "center_words, context_words = generate_training_data(tokens, context_window_size=context_window_size)\n",
    "\n",
    "context_words_idx = torch.tensor([[word2id[w] for w in row] for row in context_words])\n",
    "center_words_idx = torch.tensor([[word2id[w]] for w in center_words])\n",
    "\n",
    "model = model_cls(vocab_size=len(vocabulary), context_size=context_size, embedding_dim=embedding_dim)\n",
    "\n",
    "if isinstance(model, CBOW):\n",
    "    inputs = context_words_idx\n",
    "    targets = center_words_idx\n",
    "\n",
    "if isinstance(model, SkipGram):\n",
    "    inputs = center_words_idx\n",
    "    targets = context_words_idx\n",
    "    \n",
    "loss = train_word2vec(model, inputs, targets, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf3dc8-fa4e-444f-9341-a80d797f0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial2_task2 import plot_loss\n",
    "\n",
    "plot_loss(\n",
    "    \"Performance\", \n",
    "    [\n",
    "        (loss, \"model w/ higher context\", \"k\"), \n",
    "        (cbow_loss, \"CBOW\", \"r\"), \n",
    "        (skipgram_loss, \"SkipGram\", \"b\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a22529-db32-405c-b9a0-fdf7fc03c891",
   "metadata": {},
   "source": [
    "Does the context window affect the performance of the model? Explain in not more than 50 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68fed37-b25f-4b6e-b045-2931e4a745f8",
   "metadata": {},
   "source": [
    "Your response: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e2d1e1-cf90-4465-ad0e-2ec3e23cad37",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abfc47f-500e-4d45-b799-976345a2e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_words = [\"most\", \"romance\"]\n",
    "\n",
    "# Convert context words to their respective indices\n",
    "context_indices = torch.tensor([word2id[word] for word in context_words])\n",
    "\n",
    "# Get the model output\n",
    "log_probs = cbow_model(context_indices)\n",
    "\n",
    "# Find the word index with the highest probability\n",
    "max_prob_idx = torch.argmax(log_probs).item()\n",
    "\n",
    "# Find the word corresponding to the predicted index\n",
    "predicted_word = id2word[max_prob_idx]\n",
    "\n",
    "print(f\"The predicted center word is '{predicted_word}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251427c4-99fa-4d9b-8ff0-8773ada68b9b",
   "metadata": {},
   "source": [
    "Now it's your turn to apply a similar approach with the SkipGram model. Complete the code below to predict the context words surrounding a given center word using the SkipGram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b1fcf-ca67-485c-9c0c-5bffee9f3ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_word = \"movie\"\n",
    "\n",
    "# Convert the center word to its respective index\n",
    "center_indices = torch.tensor([word2id[center_word]])\n",
    "\n",
    "# Get the model output\n",
    "log_probs = skipgram_model(center_indices)\n",
    "\n",
    "# Find the word indices with the highest probability\n",
    "max_prob_idx = [torch.argmax(probs).item() for probs in log_probs]\n",
    "\n",
    "# Find the word corresponding to the predicted index\n",
    "predicted_words = [id2word[id] for id in max_prob_idx]\n",
    "\n",
    "print(f\"The predicted context words are {predicted_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AnNvvVCAhzJW",
   "metadata": {
    "id": "AnNvvVCAhzJW"
   },
   "source": [
    "### Task 3: Implementing RNN-LSTM classifier (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d2390-fdf1-442a-b7b0-a71e2a64ddda",
   "metadata": {},
   "source": [
    "Create a moview review dataset having all the positive and negative reviews from file 'tutorial2.txt':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GFjPgB3jD5bM",
   "metadata": {
    "id": "GFjPgB3jD5bM"
   },
   "outputs": [],
   "source": [
    "from tutorial2_task3 import load_training_data\n",
    "\n",
    "words, targets = load_training_data(\"tutorial2.txt\")\n",
    "\n",
    "print(words[0], targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aebd00c-2360-431b-9f44-11f9a5e8be0a",
   "metadata": {},
   "source": [
    "Encode and pad the input words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O44RhAXbZbtV",
   "metadata": {
    "id": "O44RhAXbZbtV"
   },
   "outputs": [],
   "source": [
    "from tutorial2_task1 import load_glove_model\n",
    "from tutorial2_task3 import generate_mappings, encode_and_pad\n",
    "\n",
    "embeddings = load_glove_model(\"glove.6B.50d.txt\")\n",
    "vocabulary, word2id, id2word = generate_mappings(embeddings)\n",
    "\n",
    "features = encode_and_pad(words, word2id, 20)\n",
    "\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b780e-0cd4-47e0-a510-7fa160077fb0",
   "metadata": {},
   "source": [
    "Split the dataset into train (80%) and test (20%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85HfGc5Kim30",
   "metadata": {
    "id": "85HfGc5Kim30"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    # TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f9bc4-07c4-4db8-ad7b-bccc473af666",
   "metadata": {},
   "source": [
    "Train a simple RNN-LSTM classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce258fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial2_task3 import train\n",
    "\n",
    "embeddings_tensor = torch.tensor(list(embeddings.values()))\n",
    "\n",
    "train(x_train, y_train, num_epochs=50, embeddings=embeddings_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot training performance (loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4h_Yq1dCjsqg",
   "metadata": {
    "id": "4h_Yq1dCjsqg"
   },
   "outputs": [],
   "source": [
    "# TODO: Get test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2695d8",
   "metadata": {},
   "source": [
    "### Task 4 Bert Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5608ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import DistanceMetric\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# 1. Load the resume dataset (resumes_train.csv)\n",
    "# TODO: Read the CSV file and store it in a DataFrame named 'df_resume'\n",
    "# HINT: Use pd.read_csv() to load the file\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "# TODO: Display the head of the dataframe to inspect the structure\n",
    "# HINT: Use df_resume.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fefdd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Encode the resumes using a pre-trained model\n",
    "# TODO: Initialize the SentenceTransformer model (use 'all-MiniLM-L6-v2') and encode the 'resume' column\n",
    "# HINT: Use model.encode() to encode the resumes and store them in 'embedding_arr'\n",
    "\n",
    "# TODO: Print the shape of 'embedding_arr' to check the dimensionality of the embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Apply PCA to reduce dimensionality\n",
    "# TODO: Apply PCA to reduce the embedding dimension to 2 components\n",
    "# HINT: Use PCA(n_components=2) and fit it to 'embedding_arr'\n",
    "\n",
    "# TODO: Print the explained variance ratio of the first two components\n",
    "# HINT: Use pca.explained_variance_ratio_\n",
    "\n",
    "# 4. Visualize the resumes using PCA components\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.grid()\n",
    "\n",
    "c = 0\n",
    "cmap = mpl.colormaps['jet']\n",
    "\n",
    "# TODO: Loop through each unique role in df_resume['role'], extract the corresponding embeddings, and plot them\n",
    "# HINT: Use plt.scatter() to plot the PCA-transformed embeddings for each role\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.9))\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Define a job query\n",
    "# TODO: Define a query as a string. This query represents the job you are searching for\n",
    "# EXAMPLE: \"Data Engineer with Apache Airflow experience\"\n",
    "\n",
    "# 6. Encode the job query using the same model\n",
    "# TODO: Encode the job query using the SentenceTransformer model and store the result in 'query_embedding'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce0c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Compute the distances between the query and resumes\n",
    "# TODO: Initialize the distance metric (Euclidean distance) and compute the distances between the query and each resume\n",
    "# HINT: Use DistanceMetric.get_metric() and pairwise() to compute the distances\n",
    "\n",
    "# TODO: Sort the resumes based on the distances and store the indices of the sorted array in 'idist_arr_sorted'\n",
    "# HINT: Use np.argsort()\n",
    "\n",
    "# 8. Display the top 10 most relevant roles\n",
    "# TODO: Print the top 10 most relevant roles based on the sorted distances\n",
    "# HINT: Use df_resume['role'].iloc[] with the sorted indices\n",
    "\n",
    "# 9. Display the most relevant resume\n",
    "# TODO: Print the resume text that is the closest match to the query\n",
    "# HINT: Use df_resume['resume'].iloc[] with the sorted indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe1dbf",
   "metadata": {},
   "source": [
    "# Resume Matching with Sentence Embeddings + PCA\n",
    "\n",
    "This cell implements the task:\n",
    "(a) Load `resumes_train.csv` with pandas.\n",
    "(b) Encode resumes using `SentenceTransformer('all-MiniLM-L6-v2')` and print embedding shape.\n",
    "(c) Apply PCA to 2 dimensions and produce a 2D scatter plot colored by `role`.\n",
    "(d) Define a job query, encode it, compute Euclidean distances to each resume embedding, and show top matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e5e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell: implement (a)-(d)\n",
    "\n",
    "# (a) Load dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "notebook_dir = os.path.dirname(r'd:\\Neural Networks for NLP Assignments\\Assignment 2\\Exercise 2 Question\\Tutorial 2\\tutorial2.ipynb')\n",
    "csv_path = os.path.join(notebook_dir, 'resumes_train.csv')\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print('Loaded', len(df), 'rows from', csv_path)\n",
    "\n",
    "# (b) Encode resumes using SentenceTransformer\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sentence-transformers', 'scikit-learn', 'matplotlib', 'seaborn'])\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(df['resume'].astype(str).tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
    "print('Embeddings shape:', embeddings.shape)\n",
    "\n",
    "# (c) PCA to 2D and scatter plot\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(embeddings)\n",
    "\n",
    "roles = df['role'].astype(str)\n",
    "unique_roles = roles.unique()\n",
    "palette = sns.color_palette('hsv', len(unique_roles))\n",
    "role_to_color = {r: palette[i] for i,r in enumerate(unique_roles)}\n",
    "colors = roles.map(role_to_color)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "for r in unique_roles:\n",
    "    mask = roles==r\n",
    "    plt.scatter(components[mask,0], components[mask,1], label=r, alpha=0.8)\n",
    "plt.legend()\n",
    "plt.title('PCA (2D) of Resume Embeddings')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n",
    "\n",
    "# (d) Job query encoding and Euclidean distances\n",
    "query = \"Data Engineer with Apache Airflow experience\"\n",
    "query_emb = model.encode([query], convert_to_numpy=True)[0]\n",
    "\n",
    "dists = np.linalg.norm(embeddings - query_emb, axis=1)\n",
    "df['distance_to_query'] = dists\n",
    "\n",
    "# Show top 5 closest resumes\n",
    "df_sorted = df.sort_values('distance_to_query').reset_index(drop=True)\n",
    "print('\\nTop 5 matches:')\n",
    "for i in range(min(5, len(df_sorted))):\n",
    "    print(f\"\\n#{i+1} Role: {df_sorted.loc[i,'role']}  Distance: {df_sorted.loc[i,'distance_to_query']:.4f}\\n\")\n",
    "    snippet = df_sorted.loc[i,'resume']\n",
    "    print(snippet[:500].replace('\\n', ' '), '...')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
